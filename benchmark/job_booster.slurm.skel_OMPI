#!/bin/bash -x
#SBATCH --partition=booster
#SBATCH --nodes=##NODES##
#SBATCH --ntasks-per-node=##TASKSPNODE##
#SBATCH --cpus-per-task=##THREADS##
#SBATCH --output=%j.stdout
#SBATCH --error=%j.stderr
#SBATCH --time=01:30:00

export OMP_NUM_THREADS=##THREADS##

module load Architecture/KNL
module load GCC
module use /homeb/zam/broemmel/jurecabooster/OpenMPI/module
module load OpenMPI

# orted or mpirun do not find themselves properly, so help them
OMPIP=`dirname $(which mpirun)`

## ### THIS IS WITHOUT BINDING/PINNING !!!!
## # construct list of hostnames for OpenMPI
## scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]}' > hosts
## 
## env -u SLURM_HOSTLIST -u SLURM_JOBID \
##    time \
##    ${OMPIP}/mpirun -np ${SLURM_NTASKS} \
##    -x OMP_NUM_THREADS \
##    -x PATH \
##    -x LD_LIBRARY_PATH \
##    -tag-output \
##    --report-bindings \
##    --bind-to none \
##    -H $(cat hosts | tr '\n' ',' | sed 's/,$//') \
##    ./pepc-benchmark ./params > log_nopin
## 


### THIS IS WITH BINDING/PINNING !!!!
# pretty please SLURM to tell us the amount of cores (SMT) to compute cores per NODE and CPU
PHYS_CORES_NODE=68
PHYS_CORES_CPU=68

# figure our the number of threads per node and decide whether we need SMT
TOTAL_THREADS=$(($SLURM_CPUS_PER_TASK * $SLURM_NTASKS_PER_NODE))
if [ "$TOTAL_THREADS" -gt "$PHYS_CORES_NODE" ]; then
   SMT_FLAG='--use-hwthread-cpus'
   MAP='node'
   if [ "$TOTAL_THREADS" -eq "$(($PHYS_CORES_NODE * 2))" ]; then
      # we have a total of 136 threads
      # construct list of hostnames for OpenMPI
      scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]*ENVIRON["SLURM_CPUS_PER_TASK"]*2}' > hosts_pin
      PE=$((${SLURM_CPUS_PER_TASK}*2))
   fi
   if [ "$TOTAL_THREADS" -eq "$(($PHYS_CORES_NODE * 4))" ]; then
      # we have a total of 272 threads
      # construct list of hostnames for OpenMPI
      scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]*ENVIRON["SLURM_CPUS_PER_TASK"]}' > hosts_pin
      PE=${SLURM_CPUS_PER_TASK}
   fi
else
   SMT_FLAG=' '
   MAP='node'
   # construct list of hostnames for OpenMPI
   scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]*ENVIRON["SLURM_CPUS_PER_TASK"]}' > hosts_pin
   PE=${SLURM_CPUS_PER_TASK}
fi

# perform actual PEPC run
env -u SLURM_HOSTLIST -u SLURM_JOBID \
   time \
   ${OMPIP}/mpirun -np ${SLURM_NTASKS} \
   -x OMP_NUM_THREADS \
   -x PATH \
   -x LD_LIBRARY_PATH \
   -tag-output \
   --report-bindings \
   -map-by ${MAP}:pe=${PE} \
   $SMT_FLAG \
   -H $(cat hosts_pin | tr '\n' ',' | sed 's/,$//') \
   ./pepc-benchmark ./params > log


## ### THIS ATTEMPTS TO CREATE A SPANNED HWTHREAD PINNING
## this throws errors
## # construct list of hostnames for OpenMPI
## scontrol show hostnames | awk '{print $0":272"}' > hosts_span
## 
## # figure our the number of threads per node and decide whether we need SMT
## SMT_FLAG='--use-hwthread-cpus'
## MAP='hwthread'
## 
## # perform actual PEPC run
## env -u SLURM_HOSTLIST -u SLURM_JOBID \
##    time \
##    ${OMPIP}/mpirun -np ${SLURM_NTASKS} \
##    -x OMP_NUM_THREADS \
##    -x PATH \
##    -x LD_LIBRARY_PATH \
##    -tag-output \
##    --report-bindings \
##    -map-by ${MAP}:pe=${SLURM_CPUS_PER_TASK}:span \
##    $SMT_FLAG \
##    -H $(cat hosts_span | tr '\n' ',' | sed 's/,$//') \
##    ./pepc-benchmark ./params > log_span
## 

touch ready

wait
