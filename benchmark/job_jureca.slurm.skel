#!/bin/bash -x
#SBATCH --partition=batch
#SBATCH --nodes=##NODES##
#SBATCH --ntasks-per-node=##TASKSPNODE##
#SBATCH --cpus-per-task=##THREADS##
#SBATCH --output=%j.stdout
#SBATCH --error=%j.stderr
#SBATCH --time=01:30:00
#SBATCH --mail-user=d.broemmel@fz-juelich.de
#SBATCH --mail-type=FAIL

# set up different module environmets, possibly set ENV variables
#---------------
if [ "##MPI##" == "intel_intel" ]; then
   module load Intel/2017.0.098-GCC-5.4.0 IntelMPI/2017.0.098
   export I_MPI_OFA_TRANSLATION_CACHE=0
   export I_MPI_DAPL_TRANSLATION_CACHE=0
   export I_MPI_FABRIC=shm:ofa
fi

#---------------
if [ "##MPI##" == "gcc_mvapich" ]; then 
   module load GCC/5.4.0 MVAPICH2/2.2-GDR
fi

#---------------
if [ "##MPI##" == "intel_mvapich" ]; then 
   module load Intel/2016.4.258-GCC-5.4.0 MVAPICH2/2.2-GDR
fi

#---------------
if [ "##MPI##" == "gcc_ps" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   module load pscom/.5.1.2-1
fi

#---------------
if [ "##MPI##" == "gcc_pssilly" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   export PSP_RENDEZVOUS_OPENIB=-1
fi

#---------------
if [ "##MPI##" == "gcc_ompi" ]; then 
   module use /usr/local/software/$(cat /etc/FZJ/systemname)/OtherStages
   module load Stages/2016b
   module use ~broemmel/$(cat /etc/FZJ/systemname)/OpenMPI/module/
   module load GCC/5.4.0 OpenMPI/2.1.0
fi

#---------------
if [ "##MPI##" == "gcc_ompi_nb" ]; then 
   module use /usr/local/software/$(cat /etc/FZJ/systemname)/OtherStages
   module load Stages/2016b
   module use ~broemmel/$(cat /etc/FZJ/systemname)/OpenMPI/module/
   module load GCC/5.4.0 OpenMPI
fi

#---------------
if [ "##MPI##" == "gcc_ps_new" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   module load pscom/.5.1.2-1
fi

#---------------
if [ "##MPI##" == "gcc_ps_pin" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   module load pscom/.5.1.2-1
fi

#---------------
if [ "##MPI##" == "gcc_ps_new_malloc" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   module load pscom/.5.1.2-1
   export MALLOC_ARENA_MAX=1
fi

#---------------
if [ "##MPI##" == "gcc_ps_old" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
fi

#---------------
if [ "##MPI##" == "gcc_ps_old_rdzv" ]; then 
   module load GCC/5.4.0 ParaStationMPI/5.1.5-1
   export PSP_RENDEZVOUS_OPENIB=-1
fi

export OMP_NUM_THREADS=##THREADS##

# now switch between complicated OpenMPI and plain psslurm integration
if [ "##MPI##" == "gcc_ompi" ]; then
   # construct list of hostnames for OpenMPI
   scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]*ENVIRON["SLURM_CPUS_PER_TASK"]}' > hosts

   # orted or mpirun do not find themselves properly, so help them
   OMPIP=`dirname $(which mpirun)`

   # pretty please SLURM to tell us the amount of cores (SMT) to compute cores per NODE and CPU
   PHYS_CORES_NODE=$(($SLURM_CPUS_ON_NODE/2))
   PHYS_CORES_CPU=$(($SLURM_CPUS_ON_NODE/4))

   # figure our the number of threads per node and decide whether we need SMT
   TOTAL_THREADS=$(($SLURM_CPUS_PER_TASK * $SLURM_NTASKS_PER_NODE))
   if [ "$TOTAL_THREADS" -gt "$PHYS_CORES_NODE" ]; then
      SMT_FLAG='--use-hwthread-cpus'
      if [ "$SLURM_CPUS_PER_TASK" -le "$PHYS_CORES_NODE" ]; then
	 MAP='numa'
      else
	 MAP='node'
      fi
   else
      SMT_FLAG=' '
      if [ "$SLURM_CPUS_PER_TASK" -le "$PHYS_CORES_CPU" ]; then
	 MAP='numa'
      else
	 MAP='node'
      fi
   fi

   # perform actual PEPC run
   env -u SLURM_HOSTLIST -u SLURM_JOBID \
      ${OMPIP}/mpirun -np ${SLURM_NTASKS} \
      -x OMP_NUM_THREADS \
      -x PATH \
      -x LD_LIBRARY_PATH \
      -tag-output \
       --report-bindings \
      -map-by ${MAP}:pe=${SLURM_CPUS_PER_TASK} \
      $SMT_FLAG \
      -H $(cat hosts | tr '\n' ',' | sed 's/,$//') \
      ./pepc-benchmark ./params > log
elif [ "##MPI##" == "gcc_ompi_nb" ]; then
   # construct list of hostnames for OpenMPI
   scontrol show hostnames | awk '{print $0":"ENVIRON["SLURM_NTASKS_PER_NODE"]}' > hosts

   # orted or mpirun do not find themselves properly, so help them
   OMPIP=`dirname $(which mpirun)`

   # perform actual PEPC run
   env -u SLURM_HOSTLIST -u SLURM_JOBID \
      ${OMPIP}/mpirun -np ${SLURM_NTASKS} \
      -x OMP_NUM_THREADS \
      -x PATH \
      -x LD_LIBRARY_PATH \
      -tag-output \
       --report-bindings \
      --bind-to none \
      -H $(cat hosts | tr '\n' ',' | sed 's/,$//') \
      ./pepc-benchmark ./params > log
elif [ "##MPI##" == "gcc_ps_pin" ]; then
   srun --cpu_bind=sockets ./show_affinity.x > affinity.log
   srun --cpu_bind=sockets -l ./pepc-benchmark ./params > log
else
   srun -l ./pepc-benchmark ./params > log
fi

touch the_eagle_has_landed

wait
